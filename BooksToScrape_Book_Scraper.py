# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14JamKMVcN5QL1kMRXHX6llTstPHxGWkl
"""

## NOTE: Runtime is still quite poor and could do with more optimisation. It took me ~5minutes to run the code however it will eventually complete
## (I have added updates for what stage it is on however it doesn't give updates while that section is running)
##
## I have partially completed improving readability. The code is divided into different functions that complete different tasks and is commented. However, there is no control from a central function that allows for interactivity
## which is something I would like to add. This would allow for users to view data through the program, view data grouped by category etc.
##
## Finally, the data is not perfectly clean. I didn't manage to get the data retrieval to clean the data for each book (i.e. make the data types correct, show the # of books available instead of it being a string etc.
## However, the data is still accurate). It could be argued that it is not my job to clean the data and i should leave that up to a data analyst ;) - If it was me however, I would appreciate the data being clean!
##
## I would make these changes should I find the time.





## Import our libraries

from bs4 import BeautifulSoup  ## the BeautifulSoup library for scraping from the bs4 package
import requests ## Establish website connection using the requests library
import pandas as pd
import numpy as np
import re ## RegEx for pattern matching
import pprint
import time

def get_cats():

  print(""" --------- UPDATE ---------

The scrapper will now gain a list of all available category pages where books are stored.

This may take a while, we appreciate your patience!
""")

  # Begin by getting a list of all categories of books and their respective links.

  categories = []

  # Create a connection to the homepage and grab the contents

  homepage = 'https://books.toscrape.com/'
  response = requests.get(homepage)

  soup = BeautifulSoup(response.text, 'html.parser')

  # Create a list of the links to each category

  Links = [y.get('href') for y in soup.find_all('a')]
  category_links = [link for link in Links if "catalogue/category" in link]
  category_links = ['https://books.toscrape.com/' + x for x in category_links]
  category_links = category_links[1:]

  # Find where the navigation panel is stored (this holds all the categories) and extract the text. Strip away the blank spaces and split it up by where the new lines are

  cats = soup.find("ul", class_ = "nav nav-list").get_text().strip().split('\n')

  # For each category we have, remove any unnecessary spaces we may have missed (or if the string is just spaces, makes it empty)
  # Then, as long as we have text in the string (therefore it being a category), we can add it to the list.

  for cat in cats:
      cat = cat.strip()
      if cat:
          categories.append(cat)

  # Remove the header of the list

  categories.remove("Books")

  # Create a dictionary storing the category and the link to that webpage of books

  cat_dict = {}

  link_count = 0
  for category in categories:
    cat_dict[category] = category_links[link_count]
    link_count += 1

  # return cat_dict
  get_books(cat_dict)


# This function is now complete. We have generated a full dictionary of categories and their respective link. This allows us to go through all categories and assign our findings to the right category.
# We will next define a function used to generate a list of dictionaries which store books, their links and their category.



def get_books(cat_dict):

    # Give the user an update

  print(""" --------- UPDATE ---------

The scrapper has retrieved a list of all categories of books available on "bookstoscrape.com.".

It will now gain information of all books listed in these categories.

This may take longer than retrieving categories. We once again ask for your patience. Thank you!
  """)

  # Initialise some variables to store data on our books.

  book_data = {}
  full_data = []
  book_base = "https://books.toscrape.com/catalogue/"

  # Iterate through the homepages of each category

  for cat, link in cat_dict.items():
    cat_site = link
    response_cat = requests.get(cat_site)
    soup = BeautifulSoup(response_cat.text, 'html.parser')

  # Get every book on the homepage of a category
  # We begin by replacing the filler with the root of the webpage in order to generate a link. We do this for all book links which we find in the specific tag.
  # Then, we remove the link to the next page (if there is one) as this is also stored in the same tags.
  # Finally, we clean up some formating errors that may occur due to differences in webpage url structure. This ensures all book links are valid.

    Books = {y.get('href').replace('../../', 'https://books.toscrape.com/catalogue/') for y in soup.find('section').findAll('a')}
    Books = {book for book in Books if not re.search(r'page-\d+\.html', book)}
    Books = {re.sub(r'\.\./', '', book) for book in Books}

  # For each book, we then define an empty dictionary that we use to store our data on this book. It is scraped from the page and then the dictionary is added to our previously defined list.

    for book in Books:
      book_data = {}
      header_name = ""
      val_val = ""


      for tr_tag in soup.find_all('tr'):
        header_name = (tr_tag.find('th').text.strip())
        val_val = (tr_tag.find('td').text.strip())

      book_data[header_name] = val_val

      book_data['Book'] = soup.find("h1").text.strip()
      book_data['Link'] = book
      book_data['Category'] = cat
      full_data.append(book_data)

    # Search the page to find if there is a button which leads to the next page. If there is, store the suffix of the link. If not, leave the variable empty. While there is a next page present, get the book link and genre
    # and repeat the scraping process from before on each book until the category is complete.

    next_page = None
    next_page_tag = soup.find('li', class_='next')
    next_page = next_page_tag.find('a')['href'] if next_page_tag else None

    while next_page:
      cat_site = cat_site.rsplit('/', 1)[0] + '/' + next_page
      response_cat = requests.get(cat_site)
      soup = BeautifulSoup(response_cat.text, 'html.parser')


      Books = {y.get('href').replace('../../', 'https://books.toscrape.com/catalogue/') for y in soup.find('section').findAll('a')}
      Books = {book for book in Books if not re.search(r'page-\d+\.html', book)}
      Books = {re.sub(r'\.\./', '', book) for book in Books}

      for book in Books:
        book_data = {}
        header_name = ""
        val_val = ""


        for tr_tag in soup.find_all('tr'):
          header_name = (tr_tag.find('th').text.strip())
          val_val = (tr_tag.find('td').text.strip())

        book_data[header_name] = val_val

        book_data['Book'] = soup.find("h1").text.strip()
        book_data['Link'] = book
        book_data['Category'] = cat
        full_data.append(book_data)

      next_page = None
      next_page_tag = soup.find('li', class_='next')
      next_page = next_page_tag.find('a')['href'] if next_page_tag else None

  #return full_data
  get_data(full_data)



# This function is now complete. We have generated a full list of dictionaries of books and their respective link. This allows us to go through all books and begin the process of scraping information from their listing pages
# that won't be available on the category pages.
# We will next define a function used to generate an exportable dataframe storing all the information on each product page including price, availability, descriptions, features etc.



def get_data(full_data):

    # Give the user an update

  print(""" --------- UPDATE ---------

The scraper has now retrieved a full list of book links on this webpage. It will now work through this list to find information on them.

For the final time, we ask for patience as this may take a while.
""")

  # Create a dataframe where we will store our book data. We begin by inserting what we currently have and turning our books into a list we can iterate through

  df = pd.DataFrame(full_data)
  all_books = df['Link'].to_list()
  full_data = []

  # For each book webpage, we generate a new connection to the site. This allows us to gain information on each book. We get info on: Price, Stock Availability, Product Description and all the features stored in the table
  # on each webpage, see https://books.toscrape.com/catalogue/sharp-objects_997/index.html for an example.
  # With the data we scrape, we add each to a dictionary which we then add into the dataframe in the same row where our connection link equals what is stored in the dataframe, aligning data with the respective book.

  for book in all_books:
    site = book
    response = requests.get(site)
    soup = BeautifulSoup(response.text, 'html.parser')

    book_data = {}
    book_data['Title'] = soup.find("h1").text.strip()
    book_data_price = soup.find("p", class_ = "price_color")
    if book_data_price:
      book_data['Price'] = book_data_price.text.strip().removeprefix('Â£')
    else:
      book_data['Price'] = np.nan
    book_data_stock = soup.find("p", class_ = "instock availability")
    if book_data_stock:
      book_data['Stock'] = book_data_stock.text.strip()
    else:
      book_data['Stock'] = np.nan
    book_data_description = soup.find("div", id = "product_description")
    if book_data_description:
      book_data['Description'] = book_data_description.find_next_sibling("p").text.strip()
    else:
      book_data['Description'] = np.nan
    book_data['Link'] = book
    for tr_tag in soup.find_all('tr'):
      header_name = (tr_tag.find('th').text.strip())
      val_val = (tr_tag.find('td').text.strip())

      book_data[header_name] = val_val
    df.loc[df['Link'] == book_data['Link'], list(book_data.keys())] = list(book_data.values())

  print(""" --------- UPDATE ---------

The scraper has now retrieved a full list of books with their respective data. We will now show the first 50 data entries for validation purposes.

Thank you for your patience!
""")

  # Finally, once we have our datafrae filled out, we export it into a csv file, removing the index
  # Some data is unnecessary in this. Price before and after tax is the same (there is never tax) so we can remove these columns. Also, book was duplicated and the easiest solution was
  # just to drop the column instead of finding where I duplicated it :)
  df.drop(columns = ("Book", "Price (excl. tax)", "Price (incl. tax)", "Tax"), inplace = True)
  df.to_csv("Scraped Books.csv", index=False)


get_cats()